<Namespace Name="AVFoundation">
  <Docs>
    <summary>The AVFoundation namespace provides high-level classes for audio recording and playback.</summary>
    <remarks>
      <para>
        This namespace contains high-level recording and playback
        capabilities for audio and video.   
      </para>
      <para>
        This library sits on top of CoreMedia, CoreAudio and CoreVideo
        but does not provide any user interface elements for UIKit.
        It is a toolkit for recording and playing back audio and
        video.
      </para>
      <para>AV Foundation uses background processing extensively. Application developers should take care to ensure thread safety and use <see cref="M:Foundation.NSObject.InvokeOnMainThread*" /> or other technique when updating their user interface.</para>
      <para>AVFoundation is not necessary for some common tasks:</para>
      <list type="table">
        <listheader>
          <term>Task</term>
          <description>Technique</description>
        </listheader>
        <item>
          <term>Display video</term>
          <description>Use Media Player's <see cref="T:MediaPlayer.MPMoviePlayerController" /> or <see cref="T:MediaPlayer.MPMoviePlayerViewController" />.</description>
        </item>
        <item>
          <term>Capture a photograph or video</term>
          <description>Use UIKit's <see cref="T:UIKit.UIImagePickerController" /></description>
        </item>
        <item>
          <term>Play audio files</term>
          <description>Use AV Foundation's <see cref="T:AVFoundation.AVAudioPlayer" />.</description>
        </item>
        <item>
          <term>Capture audio files</term>
          <description>Use AV Foundation's <see cref="T:AVFoundation.AVAudioRecorder" />, as discussed below in "Capture Audio Files".</description>
        </item>
        <item>
          <term>Complex video display or audio playback</term>
          <description>Use AV Foundation, as discussed below in "Custom Playback".</description>
        </item>
        <item>
          <term>Custom media capture</term>
          <description>Use AV Foundation, as discussed below in "Custom Media Capture".</description>
        </item>
        <item>
          <term>Media file writing, reading, and transcoding</term>
          <description>Use AV Foundation, as discussed below in "Media file writing, reading, and transcoding".</description>
        </item>
        <item>
          <term>Media editing</term>
          <description>Use UIKit's <format type="text/html"><a href="https://docs.microsoft.com/en-us/search/index?search=T:UIKit.UIVideoKitController&amp;scope=Xamarin" title="T:UIKit.UIVideoKitController">T:UIKit.UIVideoKitController</a></format> or AV Foundation building blocks.</description>
        </item>
        <item>
          <term>Barcode recognition and face detection</term>
          <description>Use AV Foundation, as discussed below in "Live Recognition".</description>
        </item>
        <item>
          <term>Speech synthesis</term>
          <description>Use AV Foundation, as discussed below in "Speech synthesis".</description>
        </item>
      </list>
      <para>An <see cref="T:AVFoundation.AVAsset" /> represents one or more media assets. These are held in its <see cref="P:AVFoundation.AVAsset.Tracks" /> property. Additionally, <see cref="T:AVFoundation.AVAsset" />s include metadata, track grouping, and preferences about the media.</para>
      <para>Because media assets such as movies are large, instantiating an <see cref="T:AVFoundation.AVAsset" /> will not automatically load the file. Properties are loaded when they are queried or via explicit calls to <see cref="M:AVFoundation.AVAsset.LoadValuesTaskAsync*" /> or <see cref="M:AVFoundation.AVAsset.LoadValuesAsynchronously*" />.</para>
      <format type="text/html">
        <h3>Capture Audio Files</h3>
      </format>
      <para>The application developer must first interaction with the static singleton Audio Session object, which mediates sound between the app and the operating system. Both <see cref="T:AudioToolbox.AudioSession" /> and <see cref="T:AVFoundation.AVAudioSession" /> refer to this same underlying singleton. Most properties in <see cref="T:AudioToolbox.AudioSession" /> are deprecated in iOS 7 and later and application developers should prefer the properties in <see cref="T:AVFoundation.AVAudioSession" />.</para>
      <list type="table">
        <listheader>
          <term>Task</term>
          <description>Using <see cref="T:AVFoundation.AVAudioSession" /></description>
          <description>Using <see cref="T:AudioToolbox.AudioSession" /></description>
        </listheader>
        <item>
          <term>Initialization</term>
          <description>
            <see cref="M:AVFoundation.AVAudioSession.SharedInstance*" /> (explicit initialization not required)</description>
          <description>
            <format type="text/html">
              <a href="https://docs.microsoft.com/en-us/search/index?search=M:AudioToolbox.AudioSession.Initialize(CFRunLoop,string)&amp;scope=Xamarin" title="M:AudioToolbox.AudioSession.Initialize(CFRunLoop,string)">M:AudioToolbox.AudioSession.Initialize(CFRunLoop,string)</a>
            </format>
          </description>
        </item>
        <item>
          <term>Set category</term>
          <description>
            <format type="text/html">
              <a href="https://docs.microsoft.com/en-us/search/index?search=M:AVFoundation.AVAudioSession.SetCategory(string, out NSError)&amp;scope=Xamarin" title="M:AVFoundation.AVAudioSession.SetCategory(string, out NSError)">M:AVFoundation.AVAudioSession.SetCategory(string, out NSError)</a>
            </format>
          </description>
          <description>
            <see cref="P:AudioToolbox.AudioSession.Category" />
          </description>
        </item>
        <item>
          <term>Set active</term>
          <description>
            <format type="text/html">
              <a href="https://docs.microsoft.com/en-us/search/index?search=M:AVFoundation.AVAudioSession.SetActive(bool, out NSError)&amp;scope=Xamarin" title="M:AVFoundation.AVAudioSession.SetActive(bool, out NSError)">M:AVFoundation.AVAudioSession.SetActive(bool, out NSError)</a>
            </format>
          </description>
          <description>
            <format type="text/html">
              <a href="https://docs.microsoft.com/en-us/search/index?search=P:AudioToolbox.AudioSession.Active&amp;scope=Xamarin" title="P:AudioToolbox.AudioSession.Active">P:AudioToolbox.AudioSession.Active</a>
            </format>
          </description>
        </item>
      </list>
      <para>The following code shows the necessary steps for preparing for audio recording.</para>
      <example>
        <code lang="C#"><![CDATA[
var session = AVAudioSession.SharedInstance();

NSError error = null;
session.SetCategory(AVAudioSession.CategoryRecord, out error);
if(error != null){
	Console.WriteLine(error);
	return;
}

session.SetActive(true, out error);
if(error != null){
	Console.WriteLine(error);
	return;
}

//Declare string for application temp path and tack on the file extension
string fileName = string.Format("Myfile{0}.aac", DateTime.Now.ToString("yyyyMMddHHmmss"));
string tempRecording = NSBundle.MainBundle.BundlePath + "/../tmp/" + fileName;

Console.WriteLine(tempRecording);
this.audioFilePath = NSUrl.FromFilename(tempRecording);

var audioSettings = new AudioSettings() {
	SampleRate = 44100.0f, 
	Format = MonoTouch.AudioToolbox.AudioFormatType.MPEG4AAC,
	NumberChannels = 1,
	AudioQuality = AVAudioQuality.High
};

//Set recorder parameters
NSError error;
recorder = AVAudioRecorder.Create(this.audioFilePath, audioSettings, out error);
if((recorder == null) || (error != null))
{
	Console.WriteLine(error);
	return false;
}

//Set Recorder to Prepare To Record
if(!recorder.PrepareToRecord())
{
	recorder.Dispose();
	recorder = null;
	return false;
}

recorder.FinishedRecording += delegate (object sender, AVStatusEventArgs e) {
	recorder.Dispose();
	recorder = null;
	Console.WriteLine("Done Recording (status: {0})", e.Status);
};

recorder.Record();          
          ]]></code>
      </example>
      <format type="text/html">
        <h3>Custom Playback</h3>
      </format>
      <para>
        <format type="text/html">
          <a href="https://docs.microsoft.com/en-us/search/index?search=T:AVFoundation.Player&amp;scope=Xamarin" title="T:AVFoundation.Player">T:AVFoundation.Player</a>
        </format> objects use <see cref="T:AVFoundation.AVPlayerItem" /> objects to play media. An <see cref="T:AVFoundation.AVPlayerItem" /> encapsulates the presentation state of an <see cref="T:AVFoundation.AVAsset" />.</para>
      <para>
        <img href="xamarin-media/AVFoundation/_images/AVFoundation.AssetPlayerItemPlayer.png" />
      </para>
      <format type="text/html">
        <h3>Custom Media Capture</h3>
      </format>
      <para>Many capture scenarios can be satisfied with the easier-to-use <see cref="T:UIKit.UIImagePickerController" /> and <see cref="T:AVFoundation.AVAudioRecorder" /> classes. More complex scenarios can use AV Foundation's <see cref="T:AVFoundation.AVCaptureSession" /> and related classes.</para>
      <para>A <see cref="T:AVFoundation.AVCaptureSession" /> will typically have one or more <see cref="T:AVFoundation.AVCaptureInput" />s and one or more <see cref="T:AVFoundation.AVCaptureOutput" />s. Each <see cref="T:AVFoundation.AVCaptureInput" /> will have a <see cref="T:AVFoundation.AVCaptureDevice" /> for a specific media type (audio or video). Each <format type="text/html"><a href="https://docs.microsoft.com/en-us/search/index?search=T:AVFoundation.AVCaptureOuput&amp;scope=Xamarin" title="T:AVFoundation.AVCaptureOuput">T:AVFoundation.AVCaptureOuput</a></format> will have a "buffer delegate" that will be repeatedly called with incoming data that it can render, write to file, analyze, etc.</para>
      <para>The following diagram and source code shows the initialization sequence of the </para>
      <format type="text/html">
        <a href="https://github.com/xamarin/monotouch-samples/tree/master/AVCaptureFrames">AVCaptureFrames Sample</a>
      </format>. <para></para><para><img href="xamarin-media/AVFoundation/_images/AVFoundation.av-capture-frames.png" /></para><example><code lang="C#"><![CDATA[
session = new AVCaptureSession () {
	SessionPreset = AVCaptureSession.PresetMedium
};

// create a device input and attach it to the session
var captureDevice = AVCaptureDevice.DefaultDeviceWithMediaType(AVMediaType.Video);
if (captureDevice == null){
	Console.WriteLine ("No captureDevice - this won't work on the simulator, try a physical device");
	return false;
}
// If you want to cap the frame rate at a given speed, in this sample: 15 frames per second
NSError error = null;
captureDevice.LockForConfiguration(out error);
if(error != null){
	Console.WriteLine(error);
	captureDevice.UnlockForConfiguration();
	return false;
}
captureDevice.ActiveVideoMinFrameDuration = new CMTime(1, 15);
captureDevice.UnlockForConfiguration();

var input = AVCaptureDeviceInput.FromDevice (captureDevice);
if (input == null){
	Console.WriteLine ("No input - this won't work on the simulator, try a physical device");
	return false;
}
session.AddInput (input);

// create a VideoDataOutput and add it to the sesion
var output = new AVCaptureVideoDataOutput () {
	VideoSettings = new AVVideoSettings (CVPixelFormatType.CV32BGRA),
};


// configure the output
queue = new MonoTouch.CoreFoundation.DispatchQueue ("myQueue");
outputRecorder = new OutputRecorder ();
output.SetSampleBufferDelegate (outputRecorder, queue);
session.AddOutput (output);

session.StartRunning ();
          
          ]]></code></example><para>Note that the <c>outputRecorder</c> is a custom subclass of <see cref="T:AVFoundation.AVCaptureVideoDataOutputSampleBufferDelegate" />. In this case, the incoming data is converted into a <see cref="T:CoreImage.CIImage" />, to which a <see cref="T:CoreImage.CIColorInvert" /> filter is applied before being sent to the display. </para><example><code lang="C#"><![CDATA[
public class OutputRecorder : AVCaptureVideoDataOutputSampleBufferDelegate {
	readonly CIColorInvert filter;

	public OutputRecorder()
	{
		filter = new CIColorInvert();
	} 
	public override void DidOutputSampleBuffer (AVCaptureOutput captureOutput, CMSampleBuffer sampleBuffer, AVCaptureConnection connection)
	{
		try {
			var image = ImageFromSampleBuffer (sampleBuffer);
			filter.Image = image;

			// Do something with the image, we just stuff it in our main view.
			AppDelegate.ImageView.BeginInvokeOnMainThread (delegate {
				AppDelegate.ImageView.Image = UIImage.FromImage(filter.OutputImage);
			});
	
			//
			// Although this looks innocent "Oh, he is just optimizing this case away"
			// this is incredibly important to call on this callback, because the AVFoundation
			// has a fixed number of buffers and if it runs out of free buffers, it will stop
			// delivering frames. 
			//	
			sampleBuffer.Dispose ();
		} catch (Exception e){
			Console.WriteLine (e);
		}
	}
	
	CIImage ImageFromSampleBuffer (CMSampleBuffer sampleBuffer)
	{
		// Get the CoreVideo image
		using (var pixelBuffer = sampleBuffer.GetImageBuffer () as CVPixelBuffer){
			// Lock the base address
			pixelBuffer.Lock (0);
			// Get the number of bytes per row for the pixel buffer
			var baseAddress = pixelBuffer.BaseAddress;
			int bytesPerRow = pixelBuffer.BytesPerRow;
			int width = pixelBuffer.Width;
			int height = pixelBuffer.Height;
			var flags = CGBitmapFlags.PremultipliedFirst | CGBitmapFlags.ByteOrder32Little;
			// Create a CGImage on the RGB colorspace from the configured parameter above
			using (var cs = CGColorSpace.CreateDeviceRGB ())
			using (var context = new CGBitmapContext (baseAddress,width, height, 8, bytesPerRow, cs, (CGImageAlphaInfo) flags))
			using (var cgImage = context.ToImage ()){
				pixelBuffer.Unlock (0);
				return cgImage;
			}
		}
	}
}          
          ]]></code></example><para>Video can be captured directly to file with <see cref="T:AVFoundation.AVCaptureMovieFileOutput" />. However, this class has no display-able data and cannot be used simultaneously with <see cref="T:AVFoundation.AVCaptureVideoDataOutput" />. Instead, application developers can use it in combination with a <see cref="T:AVFoundation.AVCaptureVideoPreviewLayer" />, as shown in the following example:</para><example><code lang="C#"><![CDATA[
var session = new AVCaptureSession();

var camera = AVCaptureDevice.DefaultDeviceWithMediaType(AVMediaType.Video);
var  mic = AVCaptureDevice.DefaultDeviceWithMediaType(AVMediaType.Audio);
if(camera == null || mic == null){
    throw new Exception("Can't find devices");
}

if(session.CanAddInput(camera)){
    session.AddInput(camera);
}
if(session.CanAddInput(mic)){
   session.AddInput(mic);
}

var layer = new AVCaptureVideoPreviewLayer(session);
layer.LayerVideoGravity = AVLayerVideoGravity.ResizeAspectFill;
layer.VideoGravity = AVCaptureVideoPreviewLayer.GravityResizeAspectFill;

var cameraView = new UIView();
cameraView.Layer.AddSublayer(layer);

var filePath = System.IO.Path.Combine( Path.GetTempPath(), "temporary.mov");
var fileUrl = NSUrl.FromFilename( filePath );

var movieFileOutput = new AVCaptureMovieFileOutput();
var recordingDelegate = new MyRecordingDelegate();
session.AddOutput(movieFileOutput);

movieFileOutput.StartRecordingToOutputFile( fileUrl, recordingDelegate);
          ]]></code></example><para>Application developers should note that the function <format type="text/html"><a href="https://docs.microsoft.com/en-us/search/index?search=T:AVFoundation.AVCaptureMovieFileOutput.StopRecording&amp;scope=Xamarin" title="T:AVFoundation.AVCaptureMovieFileOutput.StopRecording">T:AVFoundation.AVCaptureMovieFileOutput.StopRecording</a></format> is asynchronous; developers should wait until the <see cref="M:AVFoundation.AVCaptureFileOutputRecordingDelegate.FinishedRecording*" /> delegate method before manipulating the file (for instance, before saving it to the Photos album with <see cref="M:UIKit.UIVideo.SaveToPhotosAlbum*" /> or <see cref="M:AssetsLibrary.ALAssetsLibrary.WriteVideoToSavedPhotosAlbumAsync*" />).</para><format type="text/html"><h3>Media file writing, reading, and transcoding</h3></format><para>The following is the official list of supported audio formats for iOS 7: </para><list type="bullet"><item><term>AAC</term></item><item><term>Apple Lossless (ALAC)</term></item><item><term>A-law</term></item><item><term>IMA/ADPCM (IMA4)</term></item><item><term>Linear PCM</term></item><item><term>µ-law</term></item><item><term>DVI/Intel IMA ADPCM</term></item><item><term>Microsoft GSM 6.10</term></item><item><term>AES3-2003</term></item></list><para>And the following video formats:</para><list type="bullet"><item><term>H.264 video, up to 1.5 Mbps, 640 by 480 pixels, 30 frames per second, Low-Complexity version of the H.264 Baseline Profile with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</term></item><item><term>H.264 video, up to 768 Kbps, 320 by 240 pixels, 30 frames per second, Baseline Profile up to Level 1.3 with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</term></item><item><term>MPEG-4 video, up to 2.5 Mbps, 640 by 480 pixels, 30 frames per second, Simple Profile with AAC-LC audio up to 160 Kbps, 48 kHz, stereo audio in .m4v, .mp4, and .mov file formats</term></item></list><para>This list is incomplete: the iPhone 5S, for example, natively captures at 1280 x 720.</para><para>Reading a media file is done with an <see cref="T:AVFoundation.AVAssetReader" />.  As with many AV Foundation classes, this provides data in an asynchronous manner. The <see cref="P:AVFoundation.AVAssetReader.Outputs" /> property contains <see cref="T:AVFoundation.AVAssetReaderOutput" /> objects. The <see cref="M:AVFoundation.AVAssetReaderOutput.CopyNextSampleBuffer*" /> method on these objects will be called periodically as the <see cref="T:AVFoundation.AVAssetReader" /> processes the underlying <see cref="P:AVFoundation.AVAssetReader.Asset" />.</para><para>Writing a media file can be done with an <see cref="T:AVFoundation.AVAssetWriter" />, but in a media-capture session is more often done with a <see cref="T:AVFoundation.AVAudioRecorder" />, a <see cref="T:AVFoundation.AVCaptureMovieFileOutput" />, or using <see cref="T:UIKit.UIImagePickerController" />. The advantage of <see cref="T:AVFoundation.AVAssetWriter" /> is that it uses hardware encoding.</para><format type="text/html"><h3>Live Recognition</h3></format><para>iOS can recognize barcodes and faces being captured from video devices. </para><para>The following example demonstrates how to recognize QR and EAN13 barcodes. The <see cref="T:AVFoundation.AVCaptureSession" /> is configured and a <format type="text/html"><a href="https://docs.microsoft.com/en-us/search/index?search=T:AFoundation.AVCaptureMetadataOutput&amp;scope=Xamarin" title="T:AFoundation.AVCaptureMetadataOutput">T:AFoundation.AVCaptureMetadataOutput</a></format> is added to it. A <c>MyMetadataOutputDelegate</c>, a subclass of <see cref="T:AVFoundation.AVCaptureMetadataOutputObjectsDelegate" /> is assigned to its <format type="text/html"><a href="https://docs.microsoft.com/en-us/search/index?search=P:AVFoundation.AVCaptureMetadataObject.Delegate&amp;scope=Xamarin" title="P:AVFoundation.AVCaptureMetadataObject.Delegate">P:AVFoundation.AVCaptureMetadataObject.Delegate</a></format> property.</para><para>The <see cref="P:AVFoundation.AVCaptureMetadataOutput.MetadataObjectTypes" /> array must be set after the <see cref="T:AVFoundation.AVCaptureMetadataOutput" /> has been added to the <format type="text/html"><a href="https://docs.microsoft.com/en-us/search/index?search=T:AVFoundation.AVSession&amp;scope=Xamarin" title="T:AVFoundation.AVSession">T:AVFoundation.AVSession</a></format>.</para><para>This example shows a simple subclass of <see cref="T:AVFoundation.AVCaptureMetadataOutputObjectsDelegate" /> that raises an event when a barcode is recognized.</para><example><code lang="C#"><![CDATA[
session = new AVCaptureSession();
var camera = AVCaptureDevice.DefaultDeviceWithMediaType(AVMediaType.Video);
var input = AVCaptureDeviceInput.FromDevice(camera);
session.AddInput(input);
 
//Add the metadata output channel
metadataOutput = new AVCaptureMetadataOutput();
var metadataDelegate = new MyMetadataOutputDelegate();
metadataOutput.SetDelegate(metadataDelegate, DispatchQueue.MainQueue);
session.AddOutput(metadataOutput);
//Confusing! *After* adding to session, tell output what to recognize...
metadataOutput.MetadataObjectTypes = new NSString[] {
    AVMetadataObject.TypeQRCode,
    AVMetadataObject.TypeEAN13Code
};
//...etc...
public class MyMetadataOutputDelegate : AVCaptureMetadataOutputObjectsDelegate
{
    public override void DidOutputMetadataObjects(AVCaptureMetadataOutput captureOutput, AVMetadataObject[] metadataObjects, AVCaptureConnection connection)
    {
        foreach(var m in metadataObjects)
        {
            if(m is AVMetadataMachineReadableCodeObject)
            {
                MetadataFound(this, m as AVMetadataMachineReadableCodeObject);
            }
        }
    }
 
    public event EventHandler&lt;AVMetadataMachineReadableCodeObject&gt; MetadataFound = delegate {};
}
          ]]></code></example><format type="text/html"><h3>Speech Synthesis</h3></format><para>In iOS 7 and later, AV Foundation supports speech synthesis using voices that are localized for the language and locale.</para><para>In its simplest form, text-to-speech can be done with just two classes:</para><example><code lang="C#"><![CDATA[
var ss = new AVSpeechSynthesizer();
var su = new AVSpeechUtterance("Microphone check. One, two, one two.") {
	Rate = 0.25f
};
ss.SpeakUtterance(su);          
          ]]></code></example><para>The <see cref="T:AVFoundation.AVSpeechSynthesizer" /> maintains an internal queue of <see cref="T:AVFoundation.AVSpeechUtterance" />s. The queue is not accessible to application developers, but the synthesizer can be paused or stopped with <see cref="M:AVFoundation.AVSpeechSynthesizer.PauseSpeaking*" /> and <see cref="M:AVFoundation.AVSpeechSynthesizer.StopSpeaking*" />. Events such as <see cref="E:AVFoundation.AVSpeechSynthesizer.DidStartSpeechUtterance" /> or <see cref="E:AVFoundation.AVSpeechSynthesizer.WillSpeakRangeOfSpeechString" /> are opportunities for the application developer to modify previously-enqueued sequences.</para></remarks>
  </Docs>
</Namespace>
