<Namespace Name="ARKit">
  <Docs>
    <summary>The ARKit namespace provides support for augmented-reality sessions, including both high- and low-level APIs for projecting computer-generated imagery into a video stream.</summary>
    <remarks>
      <para>ARKit was added in iOS 11 and provides for mixed-reality sessions that combines camera input with computer-generated imagery that appears "attached" to the real-world.</para>
      <para>ARKit is only available on devices running A9 and more-powerful processors: essentially iPhone 6S and newer, iPad Pros, and iPads released no earlier than 2017.</para>
      <para>ARKit apps do not run in the Simulator.</para>
      <para>Developers have three choices for rendering AR scenes:</para>
      <list type="table">
        <listheader>
          <term>Class</term>
          <description>Use-case</description>
        </listheader>
        <item>
          <term>
            <see cref="T:ARKit.ARSCNView" />
          </term>
          <description>Combine SceneKit 3D geometry with video</description>
        </item>
        <item>
          <term>
            <see cref="T:ARKit.ARSCKView" />
          </term>
          <description>Combine SpriteKit 2D imagery with video</description>
        </item>
        <item>
          <term>Export "renderer:updateAtTime:" from their <see cref="T:ARKit.IARSCNViewDelegate" />.</term>
          <description>Allows complete custom rendering.</description>
        </item>
      </list>
      <format type="text/html">
        <h3>ARKit coordinate systems and transforms</h3>
      </format>
      <para>ARKit uses device motion and "visual odometry" to create a model of the device's camera and real-world "feature points" in relation to a virtual coordinate system. The coordinate
        system uses meters as its units. The virtual
        coordinate system has an origin calculated to be the camera's location at the time that the <see cref="T:ARKit.ARSession" /> was started. Location and orientation within
        ARKit are primarily represented using <see cref="T:OpenTK.NMatrix4" /> "native matrices". In the case of ARKit, these are column-major transforms:</para>
      <para>
        <img href="xamarin-media/ARKit/_images/OpenTK.NMatrix4.png" />
      </para>
      <para>Position or translation is in <see cref="P:OpenTK.Matrix4.M14" />, <see cref="P:OpenTK.Matrix4.M24" />, and <see cref="P:OpenTK.Matrix4.M34" />. 
        The 3x3 matrix defined by <see cref="P:OpenTK.Matrix4.M11" /> to <see cref="P:OpenTK.Matrix4.M33" /> is the rotation matrix. </para>
      <example>
        <code lang="C#"><![CDATA[
SCNVector3 Position(NMatrix4 m) => new SCNVector3(m.M14, m.M24, m.M34);          
    ]]></code>
      </example>
      <format type="text/html">
        <h3>Initialization</h3>
      </format>
      <para>The <see cref="T:ARKit.ARSession" /> object manages the overall augmented-reality process. The <see cref="M:ARKit.ARSession.Run*" /> method takes a 
        <see cref="T:ARKit.ARConfiguration" /> and an <see cref="T:ARKit.ARSessionRunOptions" /> object, as shown below:</para>
      <example>
        <code lang="C#"><![CDATA[
ARSCNView SceneView = ... // initialized in Storyboard, `ViewDidLoad`, etc.

// Create a session configuration
var configuration = new ARWorldTrackingConfiguration {
	PlaneDetection = ARPlaneDetection.Horizontal,
	LightEstimationEnabled = true
};

// Run the view's session
SceneView.Session.Run(configuration, ARSessionRunOptions.ResetTracking);
    ]]></code>
      </example>
      <para>Once a <see cref="T:ARKit.ARSession" /> is running, it's <see cref="P:ARKit.ARSession.CurrentFrame" /> property holds the active <see cref="T:ARKit.ARFrame" />. Because
        the system attempts to run ARKit at 60 frames per second, developers who reference the <see cref="P:ARKit.ARSession.CurrentFrame" /> must be sure to <c>Dispose</c> the 
        frame after they have lost it.</para>
      <para>The system tracks high-contrast "feature points" in the camera's view. These are available to the developer as a <see cref="T:ARKit.ARPointCloud" /> object that can be read at
        <see cref="P:ARKit.ARFrame.RawFeaturePoints" />. Generally, however, developers rely on the system to identify higher-level features, such as planes or human faces. When the system 
        identifies these higher-level features, it adds <see cref="T:ARKit.ARAnchor" /> objects whose <see cref="P:ARKit.ARAnchor.Position" /> properties are in the world-coordinate 
        system. Developers can use the <see cref="M:ARKit.ARSCNViewDelegate.DidAddNode*" />, <see cref="M:ARKit.ARSCNViewDelegate.DidUpdateNode*" />, and 
        <see cref="M:ARKit.ARSCNViewDelegate.DidRemoveNode*" /> methods to react to such events and to attach their custom geometry to real-world features. </para>
      <para>The augmented-reality coordinates are maintained using visual odometry and the device's motion manager. Experimentally, the tracking seems very solid over distances of at least tens of 
        meters in a continuous session.</para>
    </remarks>
    <related type="sample" href="https://github.com/xamarin/ios-samples/blob/master/ios11/ARKitPlacingObjects" />
    <related type="sample" href="https://github.com/xamarin/ios-samples/blob/master/ios11/ARKitSample" />
  </Docs>
</Namespace>
